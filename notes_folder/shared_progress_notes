#ordered by what I'm currently doing: ignore numbering
targets:

	-1. it's possible that we should be looking for real complex metaphors and trying to give our system enough context to understand them,
		rather than thinking of ones that come to mind which are probably conventionalized:
			my personal intuition is that adjective modification is the ideal domain, because easy to find non conventionalized metaphor

	0. qud identification: i still don't quite get it: shouldn't a 93rd qud
	be added to the categorical distribution, i.e. the trivial qud? this isn't what we're doing now

	3. world movement: conduct further investigation of this in both models: seems nice and robust

	1. GOAL: see if large sets of quds can be made to work in either model


	0. get model ready for testing on at least three domains:

		a. 2nd class manner verbs
		b. colours: plentiful source for metaphor, i think: will do some corpus investigations
		c. compounds
		d. data from similes: a is (like) b

	3. incorporate context:

	4. put statistical info into model: leon (correctly) thinks that this is feeding pragmatic
	data back into a model meant to generate pragmatic behaviour: i think it is nevertheless worth doing

	6. glove compositional thing

	5. implement a web experiment for one of the tasks: mechanical turk:
		a

	7. consider adding rationality param

FOR LEON:

	1. consider alternative rsa model
	2. let me know about pre-existing mechanical turk code i can adopt/adapt

discussion:

	task: sense disambiguation: could infer from multimodality

	a proposal: use the null model to supply quds

	gaussian initialization for qud

	rationality

	postprocessing
