for leon:

					discuss new uses of model above

					world_movement:

						i have implemented l0 world movement: all the floating words appear here: [('various', -0.0017817508783463567), ('latest', -0.0017368873265579423), ('special', -0.0016571416963913827), ('entire', -0.001603475460596654), ('beloved', -0.0015934709734450351), ('
						traditional', -0.0015734207892391875), ('mysterious', -0.001538864848251195)...]

						we should consider what an l1 without qud inference would yield, re. world movement

						world movement finding: when world movement DOES work, world appears to move in direction of good topics, regardless of polarity: e.g. "spacious" for room is an oven:

					s2:

						alt_s2: explain and show good results

						we can also minimize probability of certain things: i.e. negative utilities: for example: don't say a word that will lead l1 to value the trivial qud: 
						this would be GREAT for making sure the expression is metaphorical.


					other:
	
						why not add speaker world to possible utterances in s1, thus having a null utterance

						isolating ways in which the dynamics of rsa are benefiting us: trying to make an algorithm that finds cases of clear explaining away

						why not variational inference?

						quantum computing with democritus, quantum prob for rsa


						example: love is a poison: (['magical', 'poisonous', 'strange'], 0.0033887667),(['dangerous', 'mysterious', 'poisonous'], 0.12664874)

					experiment writeup:

					rsa logic and semantics: proofs of its behaviour

				upshot:

					improve choice of possible utterances for s1, by conditioning on qud
					put back the checks
					try no qud model and its world movement
					what is trivial favoured word

					try norming the trivial model vectors

				leon:

					variation based on possible utterance is a mixed bag:
						the furthest words by cosine distance, if 2, gives even results completely
						the nearest 100 gives same as nearest 2
					can i hand pick metaphors?
					KEY: why are both negative and positive movement in world movement representative of good words?
					nb.
					memoized s2 

				s2 results: fertile, boring:
					[('marshland', -1.3751153743694662), ('material', -1.7215099132488607), ('country', -1.7650174892376302), ('riverbed', -2.0681323802898763), ('greenery', -2.068263033671224), ('earth', -2.068263033671224), ('flat', -4.3480682170818685), ('crescent', -5.289160708231771), ('edge', -16.53417966727718), ('hillside', -30.467304209513507), ('cotton', -32.44647214774593), ('topsoil', -34.9303512371014), ('wheat', -79.9664726055096), ('continent', -111.16993329886897), ('surface', -128.16463086966976), ('land', -177.2338752544354), ('ground', -187.97380445365414), ('acre', -205.96289441947445), ('swamp', -206.61787793998226), ('flood', -228.36618421439633), ('meadow', -258.32783887748224), ('farm', -274.86482618216974), ('countryside', -283.06941602591974), ('sand', -302.0826911724041), ('grounds', -307.71361158256036), ('grassland', -314.54854200248224), ('grass', -331.27989576224786), ('vegetation', -344.7136421001385), ('floodplain', -350.6247749126385), ('landscape', -357.577777842326), ('fruit', -366.72346876029474), ('watershed', -371.402240732951), ('grain', -374.8567390239666), ('crop', -376.44188306693536), ('plateau', -380.30870435599786), ('estuary', -381.6289558208416), ('forest', -387.6516303813885), ('valley', -403.443256357951), ('silt', -419.66124341849786), ('coastline', -438.0427436626385), ('cattle', -480.0674323833416), ('mineral', -482.50169752006036), ('basin', -489.2064704692791), ('desert', -496.21565626029474), ('pasture', -627.2867622173261), ('rainfall', -665.6706123149823), ('river', -727.0879707134198), ('terrain', -897.4184150493573), ('soil', -1333.5843086040447), ('farmland', -1643.8697089946697)]


				leon upshot:

					look at larger set for l0 movement, use euclidean
					do it 

					check that target qud receives more probability with larger set

				chris meeting:

					some results: l1, s2
					have given up on identification
					here's what experiment is going to look like
					what venues are good for publication?
					what is expected for qp (expecting publication to diverge from qp)

					upshot:

						use corpus metaphor :, use AN phrase data, try doing identiciation as: on average: trivial ranks higher for literal: mean reciprocal rank, try pilot with controls; consider adding the give people words to choose: meeting chris on wednesday week; two weeks from now: experiment 2 pilot; validate that same corpus can be used for both



				noah:
					common knowledge
					presupposition
					deep learning stuff with captioning

				chris:

					key knowledge: qud is polarity agnostic

				leon:
					
					detection results
					aiming for linguistics conferences

					later:
						SGHMC: consult leon
						variational
						intention as lifted variable
						deep learning ideas for image captioning

					upshot:
						get good run back
						unit check tf_s1
						get l0 projection movement
						try having quds be most frequent, independent of pred, but not necessarily SUBJ 
						variational
						revert to old tf_rsa
						write separate tf_s1, and import it into tf_l1
						use list of five to tune: abstract to tuning file: easy
							man is lion, woman is rose, man is shark, room is furnace, love is poison
						think about qud captioning model
						think about intention modelling, and politeness

				leon:
					3 properties of the model we should consider:
						even if v high prob mass on first thing, good on following:  
								[(['hushed', 'lewd'], 0.95859522), (['fashionable', 'lewd'], 0.01060579), (['complimentary', 'hushed'], 0.009977784), (['comparable', 'fashionable'], 0.0039999997), (['elegant', 'mere'], 0.0039999997), (
								['fashionable', 'pricey'], 0.0033974373), (['mere', 'reminiscent'], 0.0019999999), (['proverbial', 'reminiscent'], 0.0019999999), (['fashionable', 'proverbial'], 0.0017737927), (['fashionable', 'hushed'], 0.0011394526), (['pricey', 'proverbial'], 0.0004521847), (['claustrophobic', 'fashioned'], 0.00022360102), (['claustrophobic', 'odd'], 0.00022360102), (['fashioned', 'intimate'], 0.00022360102), (['fashi
								oned', 'odd'], 0.00022360102), (['claustrophobic', 'intimate'], 0.00022360058), (['fashioned', 'typical'], 0.00022359121), (['intimate', 'typical'], 0.00022351445)

						there must be SOME way that polarity is represented as a function of worlds and quds, otherwise the model surely couldn't work: what is the most abstract possible relationship?

					why do we have to take samples at all? surely just a lossy procedure at this point?

					just using the adjectives as alternatives

					upshot:

						verify the expressivity point about adjectives: 

				chris:



					note that the model isn't good at getting the right direction of movement along the QUD
					how to convey the true task: choosing a good topic, not just a paraphrase: either direction of polarity

				leon:

					horrible bug
					the bit in l1_with_trivial where I normalize twice. seems wrong, no?

					in what sense is the inference joint?

				beth:
					mit

					signatures
					spending time at home
					using my units
					am i on track for courses

					upshot:

						heather burnett
						contacts: ignacio,tim dozat, ed king, natalia
						check with beth for courses, and lex sem for 

					potts upshot:

						regression:

							elmer
							read : data analysus using regression and multlivel hierachical models
							stats models: 
							t test
							make sure you gt the metadata right

						debug first experiment:

				noah:

					experiment and results
					plans for RSA: modelling intention and nonliteral meaning - missed lab meeting cause of dept meeting
					presupposition large scale and common knowledge: does noah have any thoughts?

					regression
					non qud model
					s2
					variational

					upshot:
						evaluate more qud functions
						complementary projections
						head to head compare these

						the l- posterior of which words is closest to the l1 posterior

						consider doing a weight vector over all the qud words, with l1 prior

						experiments

						experiment 1 question: phrase the question about the topic
							use: un as the opposite thing
							get people to generate good and bad one

				leon:

					how are we marginalizing out the quds in the first place?
					discuss the email i sent
					discuss noah issues

				noah:

					project proposals:
						presupposition: we try to infer common knowledge
						pragmatics for translation: replicating image captioning work but much better domain:
							l0 is p(eng|fre). s1 is 
						we attempt to produce translations while conditioning on intention:
						image captioning with quds
						exploring the inferred semantics of an end-to-end captioner
						exploring a model for learning alternatives
					internships


				boris:

					here's my hypothesis: at some point, the marker becomes reanalysed:
						is there thought to be a SYNTACTIC constraint on the object clitic appearing on intransitive verbs?
							if so, i have to argue that this is dropped
					perhaps mention that arabic article you found
					i want to make a big picture point about clitic doubling
					what are some sensible diagnostics i can run to build a syntactic analysis?
						what assumptions about clitic doubling does this challenge?

				chris:
					issue w/ noah: what is the norm?

				ignacio:
					functional programming, cat theory ideas:
						formalization of representations categorically: functors and natural transformations
						EG: your monic vs isomorphism idea for autoencoders
					discocat: stuff there
					commutative diagrams

				leon upshot:

					rating metaphors: the man is a wok
					check if the bug model is sampling 
