{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MwDrM-AMtMvq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "import torch\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.special import logsumexp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tpZXSVOwLXH7"
   },
   "outputs": [],
   "source": [
    "# x: [BATCH w, DIM VECS]: vectors to be projected\n",
    "# m: [DIM VECS, DIM PROJECTION SUBSPACE]. m is a single projection. each column vector in m is a line onto which one projects\n",
    "def get_projection(m):\n",
    "  covariance_matrix = np.dot(np.transpose(m),m)\n",
    "  inverse_covariance_matrix = np.linalg.inv(covariance_matrix)\n",
    "  #inverse_covariance_matrix = [DIM PROJECTION SUBSPACE, DIM PROJECTION SUBSPACE]\n",
    "  \n",
    "  def projection(x):\n",
    "    #x: [DIM VECS, BATCH SIZE]\n",
    "    #[DIM PROJECTION SUBSPACE, BATCH SIZE]\n",
    "    uncorrected_projection_weights = np.dot(np.transpose(m),x.T)\n",
    "    #[DIM PROJECTION SUBSPACE, BATCH SIZE]    \n",
    "    projection_weights = np.dot(inverse_covariance_matrix,uncorrected_projection_weights)\n",
    "    return np.dot(m,projection_weights).T\n",
    "  \n",
    "  return projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5IHWP_XX8PXi"
   },
   "outputs": [],
   "source": [
    "# def get_literal_listener(prior_mean, sigma1, sigma2):\n",
    "#   #prior_mean: [DIM VECS, 1]\n",
    "#   prior_mean = np.expand_dims(prior_mean,axis=1)\n",
    "#   def literal_listener(u):\n",
    "#     #u: [DIM VECS, UTTERANCE BATCH SIZE]\n",
    "#     posterior_mean = (sigma1*prior_mean + sigma2*u)*(sigma1^2*sigma2^2)/(sigma1^2+sigma2^2)\n",
    "#     return posterior_mean\n",
    "#   return literal_listener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fskw3SQoAWXD"
   },
   "outputs": [],
   "source": [
    "# commenting scheme:\n",
    "#   list(A) is a list of variables of type A\n",
    "#   [x,y,z] is an array of shape (x,y,z)\n",
    "  \n",
    "#   DIM VECS = dimension of word embedding space\n",
    "#   BATCH w = number of states w that are batched\n",
    "#   NUM utts = number of utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-nxbxynquap9"
   },
   "outputs": [],
   "source": [
    "# class contains several Bayesian pragmatic models:\n",
    "# L0_Unbatched\n",
    "# S1_Batched: fast S1, batched over w, and u (batch of u passed to L0): L0 is implicit in this S1\n",
    "# S1_Unbatched: no batching: useful for understanding the code semantics clearly and for testing the batched version\n",
    "# L1_Unbatched: no batched equivalent written\n",
    "class Pragmatic_Model:\n",
    "  # utterances : list(str). the utterance set U\n",
    "  # projections : list(str). the projection set Q\n",
    "  # vectors : dictionary: key:str,val:[dim vecs] of word embeddings\n",
    "  # sigma1: hyperparameter for L0: variance of L0 prior. See paper\n",
    "  # sigma2: hyperparameter for L0: variance of gaussian used in the semantics\n",
    "  # mu1: hyperparameter for L0: the mean of the L0 prior: e.g. \"man\" in man is a shark. \n",
    "  def __init__(self,\n",
    "              utterances,\n",
    "              projections,\n",
    "              vectors,\n",
    "              sigma1,\n",
    "              sigma2,\n",
    "              mu1):\n",
    "    \n",
    "    self.utterances=utterances\n",
    "    self.projections=projections\n",
    "    # [NUM U, DIM VECS]\n",
    "    self.utterance_vectors = np.asarray([vectors[u] for u in utterances]) \n",
    "    # [NUM Q, DIM VECS, DIM PROJECTION SUBSPACE]\n",
    "    self.projection_vectors = (np.asarray([np.asarray([vectors[word]/np.linalg.norm(vectors[word]) for word in words]).T for words in self.projections]))\n",
    "    \n",
    "    self.vectors=vectors\n",
    "    self.sigma1 = sigma1\n",
    "    self.sigma2 = sigma2\n",
    "    self.mu1 = mu1\n",
    "    \n",
    "    self.dimvecs = vectors[\"the\"].shape[0]\n",
    "    \n",
    "  # u : [DIM VECS]\n",
    "  def L0_Unbatched(self,u):\n",
    "    sigma1sq, sigma2sq = self.sigma1 ** 2, self.sigma2 ** 2\n",
    "    mu = np.divide(np.add(self.mu1/sigma1sq, u/sigma2sq),  ((1/sigma1sq) + (1/sigma2sq)))\n",
    "    sigma_base = ((1/sigma1sq) + (1/sigma2sq))**-1\n",
    "    sigma = np.diag([sigma_base] * self.dimvecs)\n",
    "    return mu,sigma\n",
    "  \n",
    "  # w : [1,DIM VECS]\n",
    "  # q : [DIM VECS, DIM PROJECTION SUBSPACE]\n",
    "  def S1_Unbatched(self,w,q):\n",
    "    \n",
    "    projection = get_projection(q)\n",
    "    # calculates a term that increases with the euclidean distance of q(w) to q(u) where q(x) is x projected onto the subspace spanned by q\n",
    "    def utility(w,projection,u):\n",
    "      l0_posterior_mu, l0_posterior_sigma = self.L0_Unbatched(u=u)\n",
    "      projected_w = projection(w)\n",
    "      projected_l0_posterior_mu = projection(l0_posterior_mu)\n",
    "      log_score = multivariate_normal(projected_l0_posterior_mu,l0_posterior_sigma).logpdf(projected_w)\n",
    "      return log_score\n",
    "    \n",
    "    unnormed_log_probs = [utility(w=w,projection=projection,u=u) for u in self.utterance_vectors]\n",
    "    norm = logsumexp(unnormed_log_probs)\n",
    "    return unnormed_log_probs - norm\n",
    "  \n",
    "  # ws: [BATCH w, DIM VECS]\n",
    "  # q: [DIM VECS, DIM PROJECTION SUBSPACE]\n",
    "  def S1_Batched(self, ws,q):\n",
    "\n",
    "    projection = get_projection(q)\n",
    "\n",
    "    # obtain L0 posterior MU and SIGMA\n",
    "    sigma1sq, sigma2sq = self.sigma1 ** 2, self.sigma2 ** 2\n",
    "    inverse_sd = (1/sigma1sq) + (1/sigma2sq)\n",
    "    sigma = np.diag([1/inverse_sd] * self.dimvecs)\n",
    "    inverse_sigma = np.linalg.inv(sigma)\n",
    "    l0_posterior_mu = np.divide(np.add(self.mu1/sigma1sq, self.utterance_vectors/sigma2sq),inverse_sd)\n",
    "\n",
    "    # projections\n",
    "    # [NUM UTTS, DIM VECS]\n",
    "    projected_l0_posterior_mu = projection(l0_posterior_mu)\n",
    "#     print(\"BATCHED\",projected_l0_posterior_mu)\n",
    "    # [BATCH w, DIM VECS]\n",
    "    projected_ws = projection(ws)\n",
    "    \n",
    "    # compute logprob of gaussian\n",
    "    # [BATCH w, NUM UTTS, DIM VECS]\n",
    "    distances = np.expand_dims(projected_ws,1)-np.expand_dims(projected_l0_posterior_mu,0)\n",
    "    # [BATCH w, NUM UTTS, DIM VECS]\n",
    "    rescaled_distances = np.einsum('ij,abi->abj',np.sqrt(inverse_sigma),distances)\n",
    "    # [BATCH w, NUM UTTS]\n",
    "    unnormed_logprobs = -0.5*np.sum(np.square(rescaled_distances),axis=2)\n",
    "    # [BATCH w,1]\n",
    "    norm = np.expand_dims(logsumexp(unnormed_logprobs,axis=-1),-1)\n",
    "    # [BATCH w, NUM UTTS]\n",
    "    return unnormed_logprobs-norm\n",
    "  \n",
    "  def L1(self,u,listener_mean):\n",
    "    #u: [DIM VECS, 1]\n",
    "    #listener_mean: [DIM VECS, 1]\n",
    "    intervals = np.arange(start=-1,stop=1,step=0.02)\n",
    "    #intervals: [1, NUM INTERVALS]\n",
    "    \n",
    "    marginal_qud_probs = np.array([])\n",
    "    for i in range(len(self.projections)):\n",
    "      q = self.projections[i]\n",
    "      #assume q is a vector of unit length\n",
    "      #q: [DIM VECS, 1]\n",
    "      \n",
    "      proj_function = get_projection(q)\n",
    "      projected_listener_prior_mean = proj_function(listener_mean)\n",
    "      #projected_listener_prior_mean: [DIM VECS, 1]\n",
    "      \n",
    "      projected_worlds = projected_listener_prior_mean + np.dot(q,intervals)\n",
    "      #projected_worlds: [DIM VECS, NUM INTERVALS]\n",
    "      \n",
    "      speaker_likelihoods = S1(projected_worlds,q)\n",
    "      #need to get likelhood of the utterance u from this\n",
    "      #assume this consists of log likelihoods\n",
    "      #speaker_likelihoods: [NUM UTTERANCES, NUM INTERVALS]\n",
    "      \n",
    "      utterance_index = get_index(u)\n",
    "      \n",
    "      speaker_likelihood_of_utterance = speaker_likelihoods[utterance_index,:]\n",
    "      #speaker_likelihood_of_utterance: [1, NUM INTERVALS]\n",
    "      \n",
    "      world_log_priors = -1/(self.sigma1^2)*intervals\n",
    "      \n",
    "      joint_probs = world_log_priors + speaker_likelihood_of_utterance\n",
    "      marginal_qud_prob = logsumexp(joint_probs)\n",
    "      \n",
    "      np.append(marginal_qud_probs,marginal_qud_prob)\n",
    "     \n",
    "    normalizing_constant = logsumexp(marginal_qud_probs)\n",
    "    marginal_qud_probs = marginal_qud_probs - normalizing_constant\n",
    "    \n",
    "    return marginal_qud_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KfakXGADuxuw"
   },
   "outputs": [],
   "source": [
    "utterances = [\"shark\",\"swimmer\"]\n",
    "projections = [[\"swims\"],[\"vicious\"]]\n",
    "ws = np.asarray([[0,3.4,4],[4,5,7],[6,7,3],[6,6,6]])\n",
    "\n",
    "simple_vecs = {}\n",
    "simple_vecs[\"swimmer\"]=np.asarray([4.0,5.0,6.0])\n",
    "simple_vecs[\"shark\"]=np.asarray([3.0,2.0,6.0])\n",
    "simple_vecs[\"man\"]=np.asarray([4.0,5.0,9.0])\n",
    "simple_vecs[\"vicious\"]=np.asarray([20.0,4.0,2.0])\n",
    "simple_vecs[\"swims\"]=np.asarray([2.0,3.0,8.0])\n",
    "simple_vecs[\"child\"]=np.asarray([0.7,-2.0,8])\n",
    "simple_vecs[\"nightmare\"]=np.asarray([5.0,5.0,7.0])\n",
    "simple_vecs[\"wonder\"]=np.asarray([5.0,8.0,-9.0])\n",
    "simple_vecs[\"the\"]=np.asarray([5.0,4.0,-9.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PZWu1YhQKfFg"
   },
   "outputs": [],
   "source": [
    "pragmatic_model = Pragmatic_Model(utterances=utterances,\n",
    "                                  projections=projections,\n",
    "                                  vectors=simple_vecs,\n",
    "                                 sigma1=1.0,\n",
    "                                 sigma2=2.0,\n",
    "                                 mu1=simple_vecs[\"man\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ZjskIUoKnFL"
   },
   "outputs": [],
   "source": [
    "#test that S1_Batched and S1_Unbatched are equivalent, up to numerical precision\n",
    "test_utterances = [\"shark\",\"swimmer\",\"wonder\",\"child\"]\n",
    "test_projections = [[\"swims\"],[\"vicious\"],[\"man\"]]\n",
    "test_mu1 = np.random.rand(5)\n",
    "test_ws = np.random.rand(10,5)\n",
    "\n",
    "test_vecs = {}\n",
    "test_vecs[\"swimmer\"]=np.random.rand(5)\n",
    "test_vecs[\"shark\"]=np.random.rand(5)\n",
    "test_vecs[\"man\"]=np.random.rand(5)\n",
    "test_vecs[\"vicious\"]=np.random.rand(5)\n",
    "test_vecs[\"swims\"]=np.random.rand(5)\n",
    "test_vecs[\"child\"]=np.random.rand(5)\n",
    "test_vecs[\"nightmare\"]=np.random.rand(5)\n",
    "test_vecs[\"wonder\"]=np.random.rand(5)\n",
    "test_vecs[\"the\"]=np.random.rand(5)\n",
    "\n",
    "test_model = Pragmatic_Model(utterances=test_utterances,\n",
    "                                  projections=test_projections,\n",
    "                                  vectors=test_vecs,\n",
    "                                 sigma1=1.0,\n",
    "                                 sigma2=2.0,\n",
    "                                 mu1=test_mu1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "colab_type": "code",
    "id": "lU1yng_ram5U",
    "outputId": "374d19fe-248e-43fc-d89e-b6f1947c163a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.419 -1.369 -1.364 -1.394] [-1.419 -1.369 -1.364 -1.394]\n",
      "[-1.393 -1.383 -1.382 -1.387] [-1.393 -1.383 -1.382 -1.387]\n",
      "[-1.411 -1.373 -1.369 -1.392] [-1.411 -1.373 -1.369 -1.392]\n",
      "[-1.428 -1.364 -1.357 -1.397] [-1.428 -1.364 -1.357 -1.397]\n",
      "[-1.457 -1.35  -1.337 -1.406] [-1.457 -1.35  -1.337 -1.406]\n",
      "[-1.374 -1.393 -1.397 -1.381] [-1.374 -1.393 -1.397 -1.381]\n",
      "[-1.385 -1.387 -1.389 -1.384] [-1.385 -1.387 -1.389 -1.384]\n",
      "[-1.426 -1.365 -1.359 -1.396] [-1.426 -1.365 -1.359 -1.396]\n",
      "[-1.422 -1.367 -1.361 -1.395] [-1.422 -1.367 -1.361 -1.395]\n",
      "[-1.36  -1.401 -1.408 -1.377] [-1.36  -1.401 -1.408 -1.377]\n"
     ]
    }
   ],
   "source": [
    "q = test_model.projection_vectors[0]\n",
    "batched = test_model.S1_Batched(ws=test_ws,q=q)\n",
    "\n",
    "for i,w in enumerate(test_ws):\n",
    "  unbatched = test_model.S1_Unbatched(w=test_ws[i],q=q)\n",
    "  b = batched[i]\n",
    "  ub = unbatched\n",
    "  print(b,ub)\n",
    "  assert(np.allclose(b,ub))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "gkCBNHCtdguZ",
    "outputId": "06206ad6-dff5-499c-b441-bc00e0961ab2"
   },
   "outputs": [],
   "source": [
    "# test that batched projection is equivalent to unbatched projection\n",
    "m = np.random.rand(10,3)\n",
    "x = np.random.rand(2,10)\n",
    "\n",
    "projection = get_projection(m)\n",
    "projected_x = projection(x)\n",
    "assert(np.allclose(projection(x)[0],projection(x[0])))\n",
    "assert(np.allclose(projection(x)[1],projection(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k_7ZGeIcE4j6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OSkQjO-A40K8"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "vector_space_pragmatics.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
