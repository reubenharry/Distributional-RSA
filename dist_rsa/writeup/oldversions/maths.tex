\documentclass[10pt,letterpaper,twocolumn]{article}
\usepackage{titlesec}
\usepackage[margin=0.8in]{geometry}
\usepackage[style=authoryear, backend=bibtex]{biblatex}
\pagenumbering{arabic}
\usepackage{lmodern}
\usepackage{graphicx}

% \usepackage{cogsci}
% \usepackage{pslatex}
\addbibresource{metaphor.bib}
\usepackage{amsmath}
\usepackage{parskip}
\usepackage{stmaryrd}
\usepackage{gb4e}
\usepackage{booktabs}
% \usepackage{hyperref}


\title{Bayesian Pragmatics with Distributional Semantics: a Computational Approach to Metaphorical Predication}
\author{Reuben Cohn-Gordon and Leon Bergen \\ Stanford}
\date{}
\titleformat*{\section}{\normalsize\bfseries}
\begin{document}
\maketitle

\section{RSA}


\begin{exe}
\ex  $L_0(w\vert u) \propto  I(u,w)*P(w)$
\end{exe}


The speaker, S$_1$ attempts, in turn, to produce the utterance u which maximizes a utility function U. For the simplest case of RSA, we define U as\footnote{Basic enrichments to this setup include the subtraction of a cost term from U(u$\vert$w), and a so-called ``rationality parameter'' which multiplies U(u$\vert$w)}:

\begin{exe}
\ex $U(u,w) = log(L_0(w\vert u))$ \label{utility}
\end{exe}

Using the utility function in (\ref{utility}) and assuming a uniform prior on utterances, we obtain a distribution for S$_1$ proportional to the utility:
\begin{exe}
\ex $S_1(u\vert w) \propto exp(U(u,w))$
\end{exe}


L$_1$, in turn, conditions on a pragmatic speaker, i.e. S$_1$, being in a world which would have produced the heard utterance:
\begin{exe}
\ex $L_1(w\vert u) \propto S_1(u\vert w)*P(w)$
\end{exe}

\section{QUD RSA}

Formally, the L$_0$ 

\begin{exe}
\ex  $L_0(w\vert u) \propto  I(f,w)*P(f\vert u)P(w)$
\end{exe}


and the S$_1$ is defined as follows:

\begin{exe}
\ex U(u,w,q) = $log(\sum_{w'} \delta_{q(w)=q(w')} * L_0(w'\vert u))$
\ex $S_1(u\vert w,q) \propto exp(U(u,w,q))$
\end{exe}

The L$_1$ then \emph{jointly infers} both a world state and a QUD\footnote{QUD RSA is one of a set of \emph{lifted variable} extensions to RSA, where the L$_m$ jointly infers both world state and other variables used in S$_n$ or L$_n$, for m $>$n. For examples of lifted variable models, see \cite{kao} and \cite{bergen}}:

\begin{exe}
\ex $L_1(w,q\vert u) \propto S_1(u\vert q,w)*P(w)*P(q)$
\end{exe}

We will, unimaginatively, refer to RSA with the joint QUD inference described here as the QUD RSA model. An example of QUD RSA in action is as follows. Let us suppose that the metaphor in question is ``The lawyer is a shark.''. We first supply a prior on possible worlds (for L$_0$ and L$_1$), on QUDs for L$_1$ and on utterances for S$_1$:

\begin{itemize}
\item World Prior: 
	\subitem $\langle$clever=True,vicious=True,aquatic=False$\rangle$ : 0.3
	\subitem $\langle$clever=True,vicious=False,aquatic=False$\rangle$ : 0.2
	\subitem $\langle$clever=False,vicious=True,aquatic=False$\rangle$ : 0.4
	\subitem $\langle$clever=False,vicious=False,aquatic=False$\rangle$ : 0.1
\item Utterance Prior:
	\subitem \emph{shark} : 0.5
	\subitem \emph{fish} : 0.5
\item QUD Prior:
	\subitem \emph{clever} ($\lambda \langle x,y,z\rangle \to x$) : 1/3
	\subitem \emph{vicious} ($\lambda \langle x,y,z\rangle \to y$) : 1/3
	\subitem \emph{breathes-underwater} ($\lambda \langle x,y,z\rangle \to z$) : 1/3
\item Features given utterance:
\subitem \emph{shark} : 
\subitem \emph{fish} :

\end{itemize}


Likewise, the next component we need to supply must also be generated by hand: the semantics. The three words of interest, \emph{shark},\emph{fish} and \emph{lawyer} are all treated as predicates, so they denote functions from worlds to truth values\footnote{We abuse notation, so that, for example, vicious(w) is true of a world w iff vicious=True in that world.}:

\begin{itemize}
\item $\llbracket \emph{shark} \rrbracket = \lambda w \to breathes-underwater(w) \wedge vicious(w)$
\item $\llbracket \emph{fish} \rrbracket = \lambda w \to breathes-underwater(w)$
\item $\llbracket \emph{lawyer} \rrbracket = \lambda w \to vicious(w) \wedge clever(w)$
\end{itemize}

We can now calculate the probability of a particular world and QUD given that we have heard \emph{shark}. 
% Note that we normalize at each step of the calculation (L$_0$,S$_1$,L$_1$), so that the output is a probability: 

$w = \langle clever=True,vicious=True,aquatic=False \rangle$

L$_1(w,\emph{vicious} \vert \emph{shark}) = $ 
% \propto 
	$ \frac{S_1(\emph{shark}\vert w, \emph{vicious}) * P(w) * P(\emph{vicious})}{\sum_w' S_1(\emph{shark}\vert w', \emph{vicious}) * P(w') * P(\emph{vicious})}$

$S_1(\emph{shark}\vert w, \emph{vicious}) = \sum_q(w)=q(w')*L_0(w'\vert \emph{shark})*P(u) $

% 		$\propto L$
% 		$ \propto $

= 0. FINISH TODO


% We can also marginalize over worlds to find that L$_1(\emph{vicious} \vert \emph{shark})$ = TODO NUMBER

 

\section{DistRSA}


\begin{exe}
\ex L$_0(w\vert u) \propto P_{\mathcal{N}}(w\vert\mu=subject,\sigma=\sigma_1)P_{\mathcal{N}}(w\vert\mu=u,\sigma=\sigma_2)$ \label{term}
% e^f$ * e$^\mu$ \label{term}
\end{exe}
The left hand term represents the prior on the world state, and the right hand term represents the observation. By the self conjugacy of Gaussians, (\ref{term}) can be reduced analytically to a single Gaussian.

The variance of the prior and observation Gaussians, $\sigma_1$ and $\sigma_2$ respectively, are hyperparameters of our model. See section (\ref{implementation}) for a discussion of their optimal values.



\begin{exe}
\ex U(u,w,q) = $log(\int_{w'} \delta_{q(w)=q(w')} * L_0(w'\vert u))$
\ex S$_1(u\vert w,q) \propto exp(U(u,w,q))$
\end{exe}


% The utility U(u$\vert\overrightarrow{\text{q}},\overrightarrow{\text{w}}$) = $\llbracket w - L_0(w'\vert u)  \rrbracket^2$ where $\llbracket \overrightarrow{\text{v}} \rrbracket^2$ represents the Euclidean norm. We can define S$_1$ in terms of this utility function as follows:

% S$_1$(u$\vert\overrightarrow{\text{q}},\overrightarrow{\text{w}}$) $\propto$ e$^{U(u\vert\overrightarrow{\text{q}},\overrightarrow{\text{w}})}$
% q(L$_0$(w$\vert$u)) * q(P(u))


\begin{exe}
\ex $L_1(w,q\vert u) \propto S_1(u\vert w,q) P_{\mathcal{N}}(w\vert\mu=subject,\sigma=\sigma_1)*P_{QUD}(q)$
\end{exe}
\end{document}